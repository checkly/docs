---
title: 'Alerting With Checkly'
description: 'Comprehensive alerting and retry strategies to minimize false positives and ensure reliable incident response when your checks fail or recover.'
sidebarTitle: 'Overview'
---



Checkly provides sophisticated alerting and retry mechanisms to keep you informed when your checks change state—whether they fail, show degraded performance, or recover. Smart configuration helps you avoid alert fatigue while ensuring critical issues reach the right people at the right time.

## What is Checkly Alerting?

Checkly alerts you when your checks transition between states such as passing to failing, degraded performance, or recovery. The system is designed to provide actionable notifications while minimizing noise through intelligent retry strategies and flexible escalation policies.

![Checkly alerting dashboard overview](/images/alerts-dash.png)

### Benefits

<Accordion title="Comprehensive Notifications">
- Configurable alert thresholds and escalation
- Multiple notification channels and integrations
- Location-based failure filtering
- SSL certificate expiration monitoring
</Accordion>

<Accordion title="Less False Positives">
- Reduce false positives with intelligent retries
- Fixed, linear, and exponential backoff options
- Location-based and network-specific retry logic
- Configurable retry limits and timeouts
</Accordion>

## Alert Settings

The [alert settings screen](https://app.checklyhq.com/alerts/settings) gives you the options to tailor when, how and how
often you want to be alerted when a check fails. This is also sometimes referred to as **threshold alerting**. For example:

- Get an alert on the second or third failure.
- Get an alert after 5 minutes of failures.
- Get one or more reminders after a failure is triggered.

Your alert notifications can be configured at three levels:

1. **Account** level: This is the default level and applies to all of your check unless you override these settings at the check level.

![alert settings account / threshold alerting](/images/docs/images/alerting/alert-settings.png)

2. **Group** level: You can explicitly override the alert settings at the group level.

![alert settings group / threshold alerting](/images/docs/images/alerting/alert-settings-group.png)

3. **Check** level: You can explicitly override the account alert settings per check. Very handy for debugging or other one-off cases.

> You can select whether group settings will override individual check settings for alerts, retries, scheduling, and location

![alert settings check / threshold alerting](/images/docs/images/alerting/alert-settings-check.png)



### Escalation

The escalation box allows you to decide when an alert should be triggered. We give you three options that are applied to all checks:

#### 1. Run based

We alert you when a check has failed a number of times consecutively. We call this a *Run Based* escalation. Note that failed checks retried
from a different region are not considered "consecutive".

Here's an example. You want to be alerted after two failures, so you set the escalation to:

![escalation when a check has failed 2 time(s)](/images/docs/images/alerting/escalation-1.png)

Your check runs in Frankfurt and Tokyo. It fails from both regions because your server is down. We consider this
one run. Five minutes later, the check fails again - assuming the check runs on a 5 minute schedule. Now we alert you.

#### 2. Time based

We alert you when a check is still failing after a period of time, regardless of the amount of check runs that are failing.
This option should mostly be used when checks are run very regularly, i.e. once every minute or five minutes.

Here's an example. You want to be alerted if your check is failing for more than 10 minutes. You set escalation to:

![escalation when a check is failing for more than 10 minutes](/images/docs/images/alerting/escalation-2.png)

Your check runs every minute. It starts failing at 15:00. Failing check results come in and are visible in the dashboard.
After 10 minutes, the clock strikes 15:10. Any failing check results that come in now trigger an alert. If the check has
resolved, no alert are triggered.

#### 3. Location failure threshold

This option can be selected in addition to the run or time-based escalation settings and only affect checks running in [parallel](/docs/monitoring/global-locations/#parallel) with two or more locations selected. 

When enabled, alerts will only be sent when the specified percentage of locations are failing. Use this setting to reduce alert noise and fatigue for services that can handle being unavailable from some locations before action is required. 

![escalation when a check is failing at more than 50% of locations](/images/docs/images/alerting/escalation-3.png)

In the example above, an alert will only be sent when the check has failed once, and at least 50% of the locations the check is running from are failing during a check run. If the check is set to run from 10 locations, and it fails in 3, no alert will be sent. If it later fails in 7 of the 10 locations an alert will be sent.

### Reminders

People are busy, life is complex. For this reason you can set up one or more reminders. With the example below, Checkly
would send you two reminders: the first reminder five minutes after the first alert is triggered and the second five minutes
after that.

![send reminder two times over ten minutes](/images/docs/images/alerting/reminders-1.png)

> When a check failure is resolved, we cancel any outstanding reminders so you don't get mixed signals.

### Muting alerts

Toggling the "mute" checkbox on a check stops the sending of all alerts but keeps the check running. This is useful when
your check might be flapping or showing other unpredictable behavior. Just mute the alerts but keep the check going while
you troubleshoot.







## Alert Channels

When adding a channel, you can select which checks to subscribe to the channel. This way you can create specific routings
for specific checks.

![alert channels](/images/docs/images/alerting/alert-channels.png)

You can also select which types of alerts should be send to your channel:

- **Failure**: When a check encounters a hard error.
- **Degradation**: When a checks is just slow, but still working.
- **Recovery**: When a check recovers from either failing or being degraded.
- [**SSL certificate expirations**](/docs/communicate/alerts/ssl-expiration/)

Configuring alert channels is mostly self explanatory except for our [advanced webhook builder](/docs/alerting/webhooks/).

After adding the channels, you either **edit** or **delete** them, or change which checks are subscribed to that specific channel.

> If you are using [Terraform](/docs/terraform-provider/) or the [CLI](/cli/overview), you will need to specify alert channel subscriptions _explicitly_ for each check / group.


## Alert States
Sending out alert notifications like emails and Slack hooks depends on four factors:

1. The **alert state** of the check, e.g. "passing", "degraded" or "failing".
2. The **transition** between these states.
3. Your **threshold alerting** preferences, e.g. "alert after two failures" or "alert after 5 minutes of failures".
4. Your **notification preferences** per alert channel.

As you can see, 1 and 2 are how Checkly works in the backend; you have no influence on this. But 3 and 4 are user configurable.
We can even add a fifth factor: if the check is muted, no alerts are send out at all.

> Note: Browser checks currently do not have a degraded state.

### States & Transitions

The following table shows all states and their transitions. There are some exceptions to some of the more complex states, 
as the history or "vector" of the state transition influences how we alert.

✅  = passing  ⚠️  = degraded  ❌  = "hard" failing 

| transition | notification | threshold  | code | notes |
|------------|----------|--------------|-------|-----------|
✅ --> ✅ | None |-|`NO_ALERT`| Nothing to see here, keep moving|   
✅ --> ⚠️ | Degraded | x|`ALERT_DEGRADED`|Send directly, if threshold is *"alert after 1 failure"*|
✅ --> ❌ | Failure  |x |`ALERT_FAILURE`|Send directly, if threshold is *"alert after 1 failure"*|
⚠️ --> ⚠️ | Degraded|x |`ALERT_DEGRADED_REMAIN` |i.e. when threshold is *"alert after 2 failures"* or *"after 5 minutes"*| 
⚠️ --> ✅ | Recovery |-|`ALERT_DEGRADED_RECOVERY`|Send but only if you received a degraded notification before|
⚠️ --> ❌ | Failure |-|`ALERT_DEGRADED_FAILURE`|This is an **escalation**, it overrides any threshold setting. We send this even if you already received degraded notifications| 
❌ --> ❌ | Failure | x|`ALERT_FAILURE_REMAIN` |i.e. when threshold is *"alert after 2 failures"* or *"after 5 minutes"*|
❌ --> ⚠️ | Degraded  |-|`ALERT_FAILURE_DEGRADED`|This is a **deescalation**, it overrides any thresholds settings. We send this even if you already received failure notifications|
❌️ --> ✅ | Recovery |-|`ALERT_RECOVERY`|Send directly|



## Escalation strategies
Control **when** and **how often** you receive notifications with threshold alerting:

- **Run-based**: Alert after consecutive failures
- **Time-based**: Alert if failing for specific duration
- **Location threshold**: Only alert when % of locations fail
- **Immediate**: Alert on first failure

## Reminder configuration

- Multiple reminder intervals
- Automatic cancellation on recovery
- Customizable reminder frequency
- Escalation to different teams

## Alert Channels
Determine **how** alert notifications reach your team:

<AccordionGroup>
<Accordion title="Communication" >
- Email notifications
- Slack integration
- SMS and phone calls
- Discord and Telegram
</Accordion>

<Accordion title="Incident management" >
- PagerDuty integration
- Opsgenie alerts
- FireHydrant incidents
- Incident.io integration
</Accordion>

<Accordion title="Custom integrations" >
- Webhook notifications
- Jira ticket creation
- Trello card creation
- Custom API endpoints
</Accordion>
</AccordionGroup>


<Note>
Effective alerting balances timely notification with alert fatigue prevention. Start with conservative settings and gradually tune based on your team's response patterns and service reliability requirements.
</Note>

<Tip>
Use Checkly's alert notification log to analyze your alerting patterns and identify opportunities to reduce noise while maintaining coverage of critical issues.
</Tip> 