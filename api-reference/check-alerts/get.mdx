---
title: 'Get Check Alerts'
openapi: 'GET /v1/check-alerts/{checkId}'
description: 'List all alerts for a specific check'
---

## Overview

This endpoint returns all alerts that have been triggered by a specific check. This is useful for analyzing the alert history and patterns for individual checks to understand their reliability and performance trends.

## Response Example

```json
{
  "data": [
    {
      "id": "alert_789123456",
      "checkId": "check_api_health",
      "checkName": "Production API Health Check",
      "checkType": "api",
      "alertType": "check_failure",
      "severity": "critical",
      "status": "resolved",
      "message": "Check failed: HTTP 500 Internal Server Error",
      "triggeredAt": "2024-01-25T14:30:00.000Z",
      "resolvedAt": "2024-01-25T14:45:00.000Z",
      "duration": 900000,
      "location": {
        "id": "us-east-1",
        "name": "N. Virginia, USA"
      },
      "checkResult": {
        "id": "result_123456789",
        "responseTime": 5240,
        "statusCode": 500,
        "error": "Internal Server Error",
        "url": "https://api.production.com/health",
        "headers": {
          "content-type": "application/json",
          "server": "nginx/1.18.0"
        },
        "body": "{\"error\": \"Database connection failed\"}"
      },
      "alertChannel": {
        "id": "channel_email_prod",
        "name": "Production Team Email",
        "type": "email",
        "target": "prod-team@example.com"
      },
      "escalationLevel": 2,
      "acknowledgedBy": {
        "id": "user_456",
        "name": "Sarah DevOps",
        "email": "sarah@example.com"
      },
      "acknowledgedAt": "2024-01-25T14:32:00.000Z",
      "resolutionNotes": "Database connection pool exhausted. Restarted database service.",
      "tags": ["api", "production", "critical", "database"]
    },
    {
      "id": "alert_456789123",
      "checkId": "check_api_health",
      "checkName": "Production API Health Check",
      "checkType": "api",
      "alertType": "performance_degradation",
      "severity": "medium",
      "status": "resolved",
      "message": "Response time exceeded threshold: 2.1s > 2.0s",
      "triggeredAt": "2024-01-24T09:15:00.000Z",
      "resolvedAt": "2024-01-24T09:22:00.000Z",
      "duration": 420000,
      "location": {
        "id": "eu-west-1",
        "name": "Ireland, Europe"
      },
      "checkResult": {
        "id": "result_456789123",
        "responseTime": 2100,
        "statusCode": 200,
        "url": "https://api.production.com/health"
      },
      "alertChannel": {
        "id": "channel_slack_ops",
        "name": "Operations Slack",
        "type": "slack",
        "target": "#ops-alerts"
      },
      "escalationLevel": 0,
      "acknowledgedBy": null,
      "acknowledgedAt": null,
      "resolutionNotes": "Performance returned to normal after CDN cache warming.",
      "tags": ["api", "production", "performance"]
    }
  ],
  "meta": {
    "checkId": "check_api_health",
    "checkName": "Production API Health Check",
    "totalAlerts": 47,
    "currentPage": 1,
    "totalPages": 5,
    "limit": 10,
    "timeframe": {
      "from": "2024-01-01T00:00:00.000Z",
      "to": "2024-01-31T23:59:59.999Z"
    },
    "summary": {
      "totalFailures": 12,
      "totalRecoveries": 12,
      "totalPerformanceDegradations": 23,
      "averageResolutionTime": 480000,
      "longestOutage": 1800000,
      "uptimePercentage": 99.1,
      "mttr": 480,
      "mttd": 120
    }
  }
}
```

## Common Use Cases

<CardGroup cols={2}>

<Card title="Check Reliability Assessment" icon="chart-bar">
Evaluate reliability and uptime of individual checks
```bash
GET /v1/check-alerts/{checkId}?period=30d&includeSummary=true
```
</Card>

<Card title="Performance Trend Analysis" icon="trending-up">
Analyze performance degradation patterns over time
```bash
GET /v1/check-alerts/{checkId}?alertType=performance_degradation
```
</Card>

<Card title="Incident Response Analysis" icon="users-cog">
Evaluate team response times and incident handling
```bash
GET /v1/check-alerts/{checkId}?severity=critical&status=resolved
```
</Card>

<Card title="Alert Optimization" icon="sliders-h">
Optimize alert thresholds and reduce false positives
```bash
GET /v1/check-alerts/{checkId}?period=90d&limit=100
```
</Card>

</CardGroup>

## Query Parameters

<AccordionGroup>

<Accordion title="Time Range Filters">
- `from` (string): Start date for alerts (ISO 8601 format)
- `to` (string): End date for alerts (ISO 8601 format)
- `period` (string): Predefined time period (24h, 7d, 30d, 90d)

**Example:**
```
?from=2024-01-01T00:00:00Z&to=2024-01-31T23:59:59Z
```

**Default:** Last 30 days if no parameters provided
</Accordion>

<Accordion title="Filtering Options">
- `alertType` (string): Filter by alert type (check_failure, check_recovery, performance_degradation)
- `severity` (string): Filter by severity level (low, medium, high, critical)
- `status` (string): Filter by alert status (triggered, acknowledged, resolved)
- `location` (string): Filter by monitoring location

**Example:**
```
?alertType=check_failure&severity=critical&status=resolved
```
</Accordion>

<Accordion title="Pagination & Sorting">
- `page` (integer): Page number (default: 1)
- `limit` (integer): Number of alerts per page (default: 10, max: 100)
- `sortBy` (string): Sort field (triggeredAt, duration, severity)
- `sortOrder` (string): Sort order (asc, desc, default: desc)
- `includeSummary` (boolean): Include alert summary statistics (default: true)

**Default:** Returns first 10 alerts sorted by most recent
</Accordion>

</AccordionGroup>

## Alert Summary Metrics

<CardGroup cols={2}>

<Card title="Reliability Metrics" icon="chart-line">
- **Uptime Percentage**: Overall availability during the time period
- **Total Failures**: Number of failure alerts
- **Total Recoveries**: Number of recovery alerts
- **MTTR**: Mean Time To Recovery (seconds)
- **MTTD**: Mean Time To Detection (seconds)
</Card>

<Card title="Performance Metrics" icon="clock">
- **Performance Degradations**: Number of performance alerts
- **Average Resolution Time**: Mean time to resolve issues
- **Longest Outage**: Duration of the longest outage
- **Escalation Rate**: Percentage of alerts that escalated
</Card>

<Card title="Alert Patterns" icon="pattern">
- **Peak Alert Hours**: Times when most alerts occur
- **Location Breakdown**: Alerts by monitoring location
- **Severity Distribution**: Breakdown of alert severities
- **Recovery Trends**: How quickly issues are resolved
</Card>

<Card title="Team Response" icon="users">
- **Acknowledgment Rate**: Percentage of alerts acknowledged
- **Response Time**: Time from alert to acknowledgment
- **Resolution Notes**: Documentation of fixes applied
- **Escalation Paths**: How alerts were escalated
</Card>

</CardGroup>

## Alert Analysis Use Cases

<AccordionGroup>

<Accordion title="Check Reliability Assessment">
Evaluate the reliability of individual checks:
- Calculate uptime percentages for SLA reporting
- Identify patterns in check failures
- Track improvement trends after optimizations
- Compare reliability across different time periods

```javascript
// Example: Calculate check reliability metrics
const alerts = response.data;
const summary = response.meta.summary;

const reliabilityScore = {
  uptime: summary.uptimePercentage,
  mttr: summary.mttr / 60, // Convert to minutes
  mttd: summary.mttd / 60, // Convert to minutes
  failureRate: (summary.totalFailures / summary.totalAlerts) * 100
};

console.log(`Check reliability: ${reliabilityScore.uptime}% uptime`);
console.log(`Average recovery time: ${reliabilityScore.mttr} minutes`);
```
</Accordion>

<Accordion title="Performance Trend Analysis">
Analyze performance degradation patterns:
- Track response time alert frequency
- Identify performance regression patterns
- Monitor seasonal performance trends
- Detect gradual performance degradation

```python
# Example: Analyze performance degradation trends
performance_alerts = [
    alert for alert in alerts 
    if alert['alertType'] == 'performance_degradation'
]

# Group by time of day to find performance patterns
from collections import defaultdict
hourly_performance_issues = defaultdict(int)

for alert in performance_alerts:
    hour = datetime.fromisoformat(alert['triggeredAt'].replace('Z', '+00:00')).hour
    hourly_performance_issues[hour] += 1

peak_hours = sorted(hourly_performance_issues.items(), key=lambda x: x[1], reverse=True)[:3]
print(f"Peak performance issue hours: {peak_hours}")
```
</Accordion>

<Accordion title="Incident Response Analysis">
Evaluate incident response effectiveness:
- Measure team response times
- Track escalation patterns
- Analyze resolution approaches
- Identify areas for process improvement

```bash
# Example: Get critical alerts with response times
curl -X GET "https://api.checklyhq.com/v1/check-alerts/check_api_health?severity=critical&includeSummary=true" \
  -H "Authorization: Bearer cu_1234567890abcdef" \
  -H "X-Checkly-Account: 550e8400-e29b-41d4-a716-446655440000"
```
</Accordion>

<Accordion title="Alert Configuration Optimization">
Optimize alert thresholds and configurations:
- Analyze false positive rates
- Evaluate alert noise levels
- Optimize severity classifications
- Fine-tune alert thresholds

```javascript
// Example: Analyze alert patterns for optimization
const alertsByType = alerts.reduce((acc, alert) => {
  const key = `${alert.alertType}_${alert.severity}`;
  if (!acc[key]) {
    acc[key] = { count: 0, avgDuration: 0, escalations: 0 };
  }
  acc[key].count++;
  acc[key].avgDuration += alert.duration || 0;
  if (alert.escalationLevel > 0) acc[key].escalations++;
  return acc;
}, {});

// Calculate optimization insights
Object.keys(alertsByType).forEach(key => {
  const data = alertsByType[key];
  data.avgDuration = data.avgDuration / data.count;
  data.escalationRate = (data.escalations / data.count) * 100;
  
  if (data.escalationRate < 5 && data.avgDuration < 300000) {
    console.log(`Consider raising threshold for ${key}: low escalation rate and quick resolution`);
  }
});
```
</Accordion>

</AccordionGroup>

## Additional Examples

<CodeGroup>

```bash cURL
curl -X GET "https://api.checklyhq.com/v1/check-alerts/check_api_health?period=30d&includeSummary=true" \
  -H "Authorization: Bearer cu_1234567890abcdef" \
  -H "X-Checkly-Account: 550e8400-e29b-41d4-a716-446655440000"
```

```javascript Node.js
const checkId = 'check_api_health';
const params = new URLSearchParams({
  period: '30d',
  includeSummary: 'true',
  limit: '50'
});

const response = await fetch(`https://api.checklyhq.com/v1/check-alerts/${checkId}?${params}`, {
  headers: {
    'Authorization': 'Bearer cu_1234567890abcdef',
    'X-Checkly-Account': '550e8400-e29b-41d4-a716-446655440000'
  }
});

const checkAlerts = await response.json();
const { data: alerts, meta } = checkAlerts;

console.log(`Check: ${meta.checkName}`);
console.log(`Total alerts in last 30 days: ${meta.totalAlerts}`);
console.log(`Uptime: ${meta.summary.uptimePercentage}%`);
console.log(`MTTR: ${Math.round(meta.summary.mttr / 60)} minutes`);

// Analyze alert frequency by day
const alertsByDay = alerts.reduce((acc, alert) => {
  const day = alert.triggeredAt.split('T')[0];
  acc[day] = (acc[day] || 0) + 1;
  return acc;
}, {});

const sortedDays = Object.entries(alertsByDay)
  .sort(([a], [b]) => a.localeCompare(b));

console.log('\nDaily alert frequency:');
sortedDays.forEach(([day, count]) => {
  console.log(`${day}: ${count} alerts`);
});

// Find unresolved alerts
const unresolvedAlerts = alerts.filter(alert => alert.status === 'triggered');
if (unresolvedAlerts.length > 0) {
  console.log(`\n⚠️ ${unresolvedAlerts.length} unresolved alerts for this check`);
}

// Performance degradation analysis
const perfAlerts = alerts.filter(alert => alert.alertType === 'performance_degradation');
if (perfAlerts.length > 0) {
  const avgResponseTime = perfAlerts.reduce((sum, alert) => 
    sum + (alert.checkResult?.responseTime || 0), 0) / perfAlerts.length;
  console.log(`\nPerformance alerts: ${perfAlerts.length}`);
  console.log(`Average response time during performance alerts: ${Math.round(avgResponseTime)}ms`);
}
```

```python Python
import requests
from datetime import datetime, timedelta
from collections import Counter

check_id = 'check_api_health'
headers = {
    'Authorization': 'Bearer cu_1234567890abcdef',
    'X-Checkly-Account': '550e8400-e29b-41d4-a716-446655440000'
}

params = {
    'period': '30d',
    'includeSummary': True,
    'limit': 100
}

response = requests.get(
    f'https://api.checklyhq.com/v1/check-alerts/{check_id}',
    headers=headers,
    params=params
)

if response.status_code == 200:
    check_alerts = response.json()
    alerts = check_alerts['data']
    meta = check_alerts['meta']
    summary = meta['summary']
    
    print(f"Check: {meta['checkName']}")
    print(f"Analysis period: {meta['timeframe']['from']} to {meta['timeframe']['to']}")
    print(f"Total alerts: {meta['totalAlerts']}")
    print(f"Uptime: {summary['uptimePercentage']}%")
    print(f"MTTR: {summary['mttr']/60:.1f} minutes")
    print(f"MTTD: {summary['mttd']/60:.1f} minutes")
    
    # Alert type breakdown
    alert_types = Counter(alert['alertType'] for alert in alerts)
    print(f"\nAlert breakdown:")
    for alert_type, count in alert_types.items():
        print(f"  {alert_type.replace('_', ' ').title()}: {count}")
    
    # Severity distribution
    severities = Counter(alert['severity'] for alert in alerts)
    print(f"\nSeverity distribution:")
    for severity, count in severities.items():
        print(f"  {severity.capitalize()}: {count}")
    
    # Location analysis
    locations = Counter(alert['location']['name'] for alert in alerts)
    print(f"\nAlerts by location:")
    for location, count in locations.most_common():
        print(f"  {location}: {count}")
    
    # Response time analysis for acknowledged alerts
    acknowledged_alerts = [alert for alert in alerts if alert['acknowledgedAt']]
    if acknowledged_alerts:
        response_times = []
        for alert in acknowledged_alerts:
            triggered = datetime.fromisoformat(alert['triggeredAt'].replace('Z', '+00:00'))
            acknowledged = datetime.fromisoformat(alert['acknowledgedAt'].replace('Z', '+00:00'))
            response_time = (acknowledged - triggered).total_seconds() / 60  # minutes
            response_times.append(response_time)
        
        avg_response_time = sum(response_times) / len(response_times)
        print(f"\nTeam response analysis:")
        print(f"  Acknowledgment rate: {len(acknowledged_alerts)/len(alerts)*100:.1f}%")
        print(f"  Average response time: {avg_response_time:.1f} minutes")
        print(f"  Fastest response: {min(response_times):.1f} minutes")
        print(f"  Slowest response: {max(response_times):.1f} minutes")
    
    # Recent trends (last 7 days vs previous 7 days)
    now = datetime.utcnow()
    week_ago = now - timedelta(days=7)
    two_weeks_ago = now - timedelta(days=14)
    
    recent_alerts = [
        alert for alert in alerts 
        if datetime.fromisoformat(alert['triggeredAt'].replace('Z', '+00:00')) >= week_ago
    ]
    previous_alerts = [
        alert for alert in alerts 
        if two_weeks_ago <= datetime.fromisoformat(alert['triggeredAt'].replace('Z', '+00:00')) < week_ago
    ]
    
    print(f"\nTrend analysis:")
    print(f"  Last 7 days: {len(recent_alerts)} alerts")
    print(f"  Previous 7 days: {len(previous_alerts)} alerts")
    trend = "improving" if len(recent_alerts) < len(previous_alerts) else "degrading" if len(recent_alerts) > len(previous_alerts) else "stable"
    print(f"  Trend: {trend}")

else:
    print(f"Error: {response.status_code} - {response.text}")
```

</CodeGroup>

<Note>
This endpoint provides detailed alert history for individual checks, enabling deep analysis of check reliability, performance patterns, and incident response effectiveness.
</Note>