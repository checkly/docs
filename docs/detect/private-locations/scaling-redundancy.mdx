---
title: 'Scaling and Redundancy'
description: 'Plan capacity, implement redundancy, and scale Checkly Private Locations for production workloads. Learn sizing calculations, scaling strategies, and high availability patterns.'
---

import { Note, Warning, Tip, Info } from 'mintlify/components'

# Scaling and Redundancy

Design resilient and scalable Private Location deployments that can handle production workloads while maintaining high availability. This guide covers capacity planning, redundancy strategies, and scaling best practices.

## Redundancy Requirements

<Note>
**Minimum recommendation**: Deploy at least 2 agents per Private Location to ensure redundancy and continuous operation.
</Note>

Checkly agents are stateless and designed for horizontal scaling. When one agent fails, others automatically handle the workload. Failed checks are automatically retried on healthy agents after a 300-second timeout.

### Failure Scenarios

<CardGroup cols={2}>
<Card title="Single Agent Failure" icon="exclamation-triangle">
**Impact**: Reduced capacity, no service interruption
**Recovery**: Automatic load redistribution to healthy agents
**Action**: Monitor and replace failed agent
</Card>

<Card title="All Agents Down" icon="x-circle">
**Impact**: No checks execute from this location
**Recovery**: Checks remain queued for 6 minutes
**Action**: Restore at least one agent within 6 minutes
</Card>
</CardGroup>

### Check Queuing Behavior

When agents are unavailable or overloaded:
- **Checks queue for up to 6 minutes** waiting for available capacity
- **Queued checks older than 6 minutes are dropped** and marked as failed
- **New agents immediately process queued checks** when they come online

## Capacity Planning

### Memory Requirements by Check Type

Different check types have varying memory requirements:

<AccordionGroup>
<Accordion title="Browser Checks">
**Memory per concurrent check**: ~1.5GB
**Includes**: Chromium browser, page content, JavaScript execution

**Example sizing**:
```bash
# For 4GB container running only browser checks
JOB_CONCURRENCY = 4GB ÷ 1.5GB = 2 (rounded down)

# For 8GB container
JOB_CONCURRENCY = 8GB ÷ 1.5GB = 5
```

**Configuration**:
```bash
docker run \
  -e API_KEY="pl_key..." \
  -e JOB_CONCURRENCY=5 \
  --memory="8g" \
  -d checkly/agent:latest
```
</Accordion>

<Accordion title="API Checks">
**Memory per concurrent check**: ~150MB
**Includes**: HTTP client, response processing, assertion execution

**Example sizing**:
```bash
# For 1GB container running only API checks
JOB_CONCURRENCY = 1GB ÷ 0.15GB = 6

# For 1.5GB container (maximum concurrency)
JOB_CONCURRENCY = 1.5GB ÷ 0.15GB = 10 (max allowed)
```

**Configuration**:
```bash
docker run \
  -e API_KEY="pl_key..." \
  -e JOB_CONCURRENCY=10 \
  --memory="2g" \
  -d checkly/agent:latest
```
</Accordion>

<Accordion title="Mixed Workloads">
**Strategy**: Size for the most memory-intensive check type (browser checks)

**Conservative approach**:
```bash
# Assume all concurrent slots could be browser checks
JOB_CONCURRENCY = Container_Memory ÷ 1.5GB
```

**If you know your workload distribution**:
```bash
# Example: 70% API checks, 30% browser checks
# For 6GB container with JOB_CONCURRENCY=4:
# Worst case: 4 browser checks = 6GB ✓
# Typical case: 3 API + 1 browser = 1.95GB ✓
```
</Accordion>
</AccordionGroup>

### Execution Time Considerations

Factor in check execution times when planning capacity:

| Check Type | Max Execution Time | Typical Duration |
|------------|-------------------|-----------------|
| API checks | 30 seconds | 1-5 seconds |
| Browser checks | 4 minutes | 30-60 seconds |
| TCP checks | 30 seconds | 1-3 seconds |
| Heartbeat checks | N/A (passive) | N/A |

### Scaling Formula

Calculate the number of agents needed:

```
Required Agents = ((Checks per minute × Average duration in minutes) ÷ JOB_CONCURRENCY) + 1
```

**Components**:
- **Checks per minute**: Total check frequency across all checks
- **Average duration**: Weighted average of actual execution times
- **JOB_CONCURRENCY**: Concurrent slots per agent
- **+1**: Redundancy buffer for failures and rolling updates

### Sizing Examples

<AccordionGroup>
<Accordion title="API-Heavy Workload">
**Scenario**: 50 API checks running every minute, average 2-second duration

**Calculation**:
```
Checks per minute: 50
Average duration: 2 seconds = 0.033 minutes
JOB_CONCURRENCY: 10 (for 2GB agent)

Required agents = ((50 × 0.033) ÷ 10) + 1
                = (1.65 ÷ 10) + 1
                = 0.165 + 1
                = 1.165 → 2 agents minimum
```

**Recommended deployment**: 2-3 agents with 2GB memory each
</Accordion>

<Accordion title="Browser-Heavy Workload">
**Scenario**: 20 browser checks every 5 minutes, average 45-second duration

**Calculation**:
```
Checks per minute: 20 ÷ 5 = 4
Average duration: 45 seconds = 0.75 minutes
JOB_CONCURRENCY: 3 (for 6GB agent)

Required agents = ((4 × 0.75) ÷ 3) + 1
                = (3 ÷ 3) + 1
                = 1 + 1
                = 2 agents minimum
```

**Recommended deployment**: 3 agents with 6GB memory each
</Accordion>

<Accordion title="Mixed Production Workload">
**Scenario**: 100 API checks (every 30 seconds) + 30 browser checks (every 2 minutes)

**Calculation**:
```
API checks per minute: 100 ÷ 0.5 = 200
Browser checks per minute: 30 ÷ 2 = 15
Total checks per minute: 215

Weighted average duration:
- API: 200 × 0.05 minutes = 10 check-minutes
- Browser: 15 × 0.75 minutes = 11.25 check-minutes
- Total: 21.25 check-minutes ÷ 215 checks = 0.099 minutes

JOB_CONCURRENCY: 4 (for 8GB agent, sized for browser checks)

Required agents = ((215 × 0.099) ÷ 4) + 1
                = (21.3 ÷ 4) + 1
                = 5.3 + 1
                = 6.3 → 7 agents
```

**Recommended deployment**: 8 agents with 8GB memory each
</Accordion>
</AccordionGroup>

## Scaling Strategies

### Vertical Scaling (Scale Up)

Increase resources for existing agents:

<CardGroup cols={2}>
<Card title="Pros" icon="arrow-up">
- Fewer containers to manage
- Reduced network overhead
- Simpler monitoring
- Lower licensing costs (if applicable)
</Card>

<Card title="Cons" icon="arrow-down">
- Single points of failure
- Larger blast radius
- Resource allocation inefficiency
- Limited by node capacity
</Card>
</CardGroup>

**When to scale up**:
- Consistent workload patterns
- Sufficient node resources available
- Cost optimization priorities
- Simplified operations preferred

### Horizontal Scaling (Scale Out)

Deploy more agent instances:

<CardGroup cols={2}>
<Card title="Pros" icon="arrow-right">
- Better fault tolerance
- Flexible resource allocation
- Easier to match workload patterns
- Better Kubernetes integration
</Card>

<Card title="Cons" icon="arrow-left">
- More containers to manage
- Increased orchestration complexity
- Higher operational overhead
- Potential resource fragmentation
</Card>
</CardGroup>

**When to scale out**:
- Variable workload patterns
- High availability requirements
- Kubernetes-native deployments
- Multi-tenant environments

## High Availability Patterns

### Multi-Zone Deployment

```yaml
# Kubernetes deployment across availability zones
apiVersion: apps/v1
kind: Deployment
metadata:
  name: checkly-agent
spec:
  replicas: 6
  template:
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - checkly-agent
              topologyKey: topology.kubernetes.io/zone
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 50
            preference:
              matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                - us-west-2a
                - us-west-2b
                - us-west-2c
```

### Rolling Update Strategy

```yaml
# Zero-downtime updates
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  template:
    spec:
      terminationGracePeriodSeconds: 60
      containers:
      - name: checkly-agent
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 30"]
```

### Circuit Breaker Pattern

```bash
#!/bin/bash
# health-check-with-circuit-breaker.sh

AGENT_NAME="checkly-agent"
FAILURE_THRESHOLD=3
FAILURE_COUNT=0
CIRCUIT_OPEN=false

check_agent_health() {
  if docker exec $AGENT_NAME curl -f -s http://localhost:8080/health; then
    FAILURE_COUNT=0
    CIRCUIT_OPEN=false
    return 0
  else
    ((FAILURE_COUNT++))
    if [ $FAILURE_COUNT -ge $FAILURE_THRESHOLD ]; then
      CIRCUIT_OPEN=true
      echo "Circuit breaker opened - restarting agent"
      docker restart $AGENT_NAME
      FAILURE_COUNT=0
    fi
    return 1
  fi
}
```

## Auto-Scaling Implementation

### Kubernetes Horizontal Pod Autoscaler

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: checkly-agent-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: checkly-agent
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 600
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

### Custom Metrics Scaling

```yaml
# Scale based on queue length (requires custom metrics)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: checkly-agent-queue-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: checkly-agent
  minReplicas: 2
  maxReplicas: 50
  metrics:
  - type: External
    external:
      metric:
        name: checkly_queue_length
        selector:
          matchLabels:
            private_location: "production-east"
      target:
        type: Value
        value: "10"  # Scale up when queue > 10 items
```

### Docker Swarm Auto-Scaling

```yaml
# docker-compose.yml with auto-scaling
version: '3.8'
services:
  checkly-agent:
    image: checkly/agent:latest
    environment:
      - API_KEY=${CHECKLY_API_KEY}
      - JOB_CONCURRENCY=5
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 30s
        order: start-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          memory: 4G
          cpus: '2'
        reservations:
          memory: 2G
          cpus: '1'
```

## Monitoring and Alerting

### Key Metrics to Monitor

<AccordionGroup>
<Accordion title="Agent Health Metrics">
- **Agent count**: Number of connected agents per Private Location
- **Agent uptime**: How long agents have been running
- **Connection status**: Agent connectivity to Checkly platform
- **Resource utilization**: CPU, memory, and disk usage

**Prometheus queries**:
```promql
# Agent count
count(up{job="checkly-agent"} == 1)

# High memory usage agents
(container_memory_usage_bytes / container_spec_memory_limit_bytes) > 0.8
```
</Accordion>

<Accordion title="Workload Metrics">
- **Queue length**: Number of checks waiting for execution
- **Check execution rate**: Checks processed per minute
- **Average response time**: Time to complete checks
- **Failure rate**: Percentage of failed check executions

**Custom metrics collection**:
```bash
# Export metrics from agent container
docker exec checkly-agent cat /proc/meminfo | grep MemAvailable
docker exec checkly-agent ps aux | grep node | awk '{print $3, $4}'
```
</Accordion>

<Accordion title="Business Impact Metrics">
- **SLA compliance**: Percentage of checks meeting SLA targets
- **Downtime duration**: Total time Private Location was unavailable
- **Check coverage**: Percentage of scheduled checks that executed
- **Alert response time**: Time to detect and respond to failures

**Sample dashboard queries**:
```sql
-- Check success rate over time
SELECT 
  time_bucket('5 minutes', executed_at) as time,
  avg(case when status = 'success' then 1 else 0 end) * 100 as success_rate
FROM check_results 
WHERE private_location = 'production-east'
GROUP BY time
ORDER BY time;
```
</Accordion>
</AccordionGroup>

### Alerting Rules

```yaml
# prometheus-alerts.yaml
groups:
- name: checkly-agents
  rules:
  - alert: ChecklyAgentDown
    expr: up{job="checkly-agent"} == 0
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "Checkly agent {{ $labels.instance }} is down"
      
  - alert: ChecklyAgentLowCount
    expr: count(up{job="checkly-agent"} == 1) < 2
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Less than 2 Checkly agents running"
      
  - alert: ChecklyAgentHighMemory
    expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) > 0.9
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Checkly agent memory usage > 90%"
      
  - alert: ChecklyAgentQueueBacklog
    expr: checkly_queue_length > 20
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "Checkly agent queue backlog detected"
```

## Disaster Recovery

### Backup and Recovery Strategy

<AccordionGroup>
<Accordion title="Configuration Backup">
**What to backup**:
- Agent configuration files
- API keys and secrets
- Deployment manifests
- Network configuration
- Monitoring setup

**Backup script example**:
```bash
#!/bin/bash
# backup-private-location-config.sh

BACKUP_DIR="/backup/checkly-$(date +%Y%m%d)"
mkdir -p $BACKUP_DIR

# Backup Kubernetes configs
kubectl get deployment checkly-agent -n checkly -o yaml > $BACKUP_DIR/deployment.yaml
kubectl get secret checkly-api-key -n checkly -o yaml > $BACKUP_DIR/secret.yaml
kubectl get configmap -n checkly -o yaml > $BACKUP_DIR/configmaps.yaml

# Backup Docker Compose files
cp docker-compose.yml $BACKUP_DIR/
cp .env $BACKUP_DIR/env.backup

# Backup monitoring configs
cp prometheus-rules.yaml $BACKUP_DIR/
cp grafana-dashboard.json $BACKUP_DIR/
```
</Accordion>

<Accordion title="Recovery Procedures">
**RTO (Recovery Time Objective)**: < 15 minutes
**RPO (Recovery Point Objective)**: < 5 minutes

**Recovery steps**:
1. **Assess impact**: Determine scope of failure
2. **Restore configuration**: Apply backed-up manifests
3. **Verify connectivity**: Ensure agents connect to Checkly
4. **Resume monitoring**: Confirm checks are executing
5. **Monitor performance**: Watch for any degradation

**Automated recovery**:
```bash
#!/bin/bash
# disaster-recovery.sh

echo "Starting disaster recovery for Private Location..."

# Restore from latest backup
LATEST_BACKUP=$(ls -td /backup/checkly-* | head -1)
echo "Restoring from: $LATEST_BACKUP"

# Apply Kubernetes configurations
kubectl apply -f $LATEST_BACKUP/deployment.yaml
kubectl apply -f $LATEST_BACKUP/secret.yaml
kubectl apply -f $LATEST_BACKUP/configmaps.yaml

# Wait for pods to be ready
kubectl wait --for=condition=ready pod -l app=checkly-agent -n checkly --timeout=300s

echo "Recovery completed. Verifying agent connectivity..."
kubectl logs -l app=checkly-agent -n checkly --tail=10
```
</Accordion>
</AccordionGroup>

### Multi-Region Redundancy

For critical workloads, consider deploying Private Locations across multiple regions:

```yaml
# Primary region (us-west-2)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: checkly-agent-primary
  labels:
    region: us-west-2
    role: primary
spec:
  replicas: 3

---
# Secondary region (us-east-1)  
apiVersion: apps/v1
kind: Deployment
metadata:
  name: checkly-agent-secondary
  labels:
    region: us-east-1
    role: secondary
spec:
  replicas: 2
```

## Cost Optimization

### Resource Right-Sizing

<AccordionGroup>
<Accordion title="Monitor Resource Usage">
```bash
# Weekly resource utilization report
kubectl top pods -n checkly --no-headers | \
awk '{print $1, $2, $3}' | \
sort -k3 -hr > weekly-resource-report.txt

# Memory utilization analysis
kubectl exec deployment/checkly-agent -n checkly -- \
  sh -c 'while true; do date; free -h; sleep 300; done' > memory-usage.log
```
</Accordion>

<Accordion title="Optimize Agent Sizing">
**Under-utilized agents** (CPU < 30%, Memory < 50%):
- Reduce resource requests/limits
- Increase JOB_CONCURRENCY if memory allows
- Consider smaller instance types

**Over-utilized agents** (CPU > 80%, Memory > 85%):
- Increase resource limits
- Reduce JOB_CONCURRENCY
- Scale horizontally instead of vertically

**Example optimization**:
```yaml
# Before: Over-provisioned
resources:
  requests:
    memory: "4Gi"
    cpu: "2"
  limits:
    memory: "8Gi"
    cpu: "4"

# After: Right-sized based on monitoring
resources:
  requests:
    memory: "2Gi"
    cpu: "1"
  limits:
    memory: "4Gi"
    cpu: "2"
```
</Accordion>
</AccordionGroup>

### Spot Instance Strategy

```yaml
# Use spot instances for cost savings (Kubernetes)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: checkly-agent-spot
spec:
  replicas: 4
  template:
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: "spot"
      tolerations:
      - key: "spot-instance"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
      # Ensure graceful shutdown
      terminationGracePeriodSeconds: 120
```

## Best Practices Summary

<CardGroup cols={2}>
<Card title="Capacity Planning" icon="calculator">
- Always provision redundancy (+1 agent minimum)
- Size for peak workload, not average
- Monitor actual resource usage regularly
- Plan for check execution time variations
</Card>

<Card title="High Availability" icon="shield-check">
- Deploy across multiple availability zones
- Implement rolling update strategies
- Use pod disruption budgets
- Monitor agent health continuously
</Card>

<Card title="Scaling" icon="arrows-maximize">
- Start small and scale based on metrics
- Prefer horizontal over vertical scaling
- Implement automated scaling policies
- Test scaling scenarios regularly
</Card>

<Card title="Cost Optimization" icon="dollar-sign">
- Right-size resources based on monitoring
- Use spot instances where appropriate
- Implement resource quotas and limits
- Regular capacity reviews and adjustments
</Card>
</CardGroup>

## Next Steps

<CardGroup cols={2}>
<Card title="Agent Configuration" href="/docs/getting-started/platform/private-locations/agent-configuration">
Advanced configuration options for optimizing agent performance
</Card>

<Card title="Monitoring Setup" href="/guides/monitoring-setup">
Implement comprehensive monitoring for your Private Locations
</Card>

<Card title="Cost Management" href="/guides/cost-optimization">
Strategies for optimizing Private Location costs
</Card>

<Card title="Performance Tuning" href="/guides/performance-tuning">
Fine-tune Private Location performance for your workloads
</Card>
</CardGroup>

<Tip>
**Start with the minimum viable deployment** (2 agents with basic resource allocation) and scale based on actual usage patterns and performance metrics.
</Tip>