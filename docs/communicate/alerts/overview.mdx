---
title: 'Alerting and Retries Overview'
description: 'Comprehensive alerting and retry strategies to minimize false positives and ensure reliable incident response when your checks fail or recover.'
sidebarTitle: 'Overview'
---



Checkly provides sophisticated alerting and retry mechanisms to keep you informed when your checks change state—whether they fail, show degraded performance, or recover. Smart configuration helps you avoid alert fatigue while ensuring critical issues reach the right people at the right time.

## What is Checkly Alerting?

Checkly alerts you when your checks transition between states such as passing to failing, degraded performance, or recovery. The system is designed to provide actionable notifications while minimizing noise through intelligent retry strategies and flexible escalation policies.

![Checkly alerting dashboard overview](/images/alerts-dash.png)

### Benefits

<Accordion title="Smart alerting">
- Configurable alert thresholds and escalation
- Multiple notification channels and integrations
- Location-based failure filtering
- SSL certificate expiration monitoring
</Accordion>

<Accordion title="Retry strategies">
- Reduce false positives with intelligent retries
- Fixed, linear, and exponential backoff options
- Location-based and network-specific retry logic
- Configurable retry limits and timeouts
</Accordion>


## Escalation strategies
Control **when** and **how often** you receive notifications with threshold alerting:

- **Run-based**: Alert after consecutive failures
- **Time-based**: Alert if failing for specific duration
- **Location threshold**: Only alert when % of locations fail
- **Immediate**: Alert on first failure

## Reminder configuration

- Multiple reminder intervals
- Automatic cancellation on recovery
- Customizable reminder frequency
- Escalation to different teams

## Alert Channels
Determine **how** alert notifications reach your team:

<AccordionGroup>
<Accordion title="Communication" >
- Email notifications
- Slack integration
- SMS and phone calls
- Discord and Telegram
</Accordion>

<Accordion title="Incident management" >
- PagerDuty integration
- Opsgenie alerts
- FireHydrant incidents
- Incident.io integration
</Accordion>

<Accordion title="Custom integrations" >
- Webhook notifications
- Jira ticket creation
- Trello card creation
- Custom API endpoints
</Accordion>
</AccordionGroup>

## Alert States and Transitions

Understanding how Checkly tracks check states helps you configure appropriate alerting:

<Steps>
<Step title="Check Execution">
Your check runs according to its schedule from configured locations
</Step>

<Step title="State Evaluation">
Checkly evaluates the check result against performance thresholds and assertions
</Step>

<Step title="State Transition">
If the check state changes (pass → fail, fail → pass, normal → degraded), the alert system activates
</Step>

<Step title="Retry Logic">
If configured, failed checks are automatically retried according to your retry strategy
</Step>

<Step title="Alert Escalation">
If retries don't resolve the issue, alerts are sent based on your escalation settings
</Step>

<Step title="Recovery Notification">
When checks recover, recovery alerts are sent according to your notification preferences
</Step>
</Steps>

### Alert Types and States

| Alert Type | Description | When Triggered |
|------------|-------------|----------------|
| `ALERT_FAILURE` | Check hard failure | Check encounters error or assertion failure |
| `ALERT_DEGRADED` | Performance degradation | Check succeeds but exceeds response time thresholds |
| `ALERT_RECOVERY` | Service recovery | Failed or degraded check returns to passing state |
| `ALERT_DEGRADED_RECOVERY` | Performance improvement | Degraded check returns to normal performance |
| `ALERT_SSL` | SSL certificate expiration | Certificate expires within warning threshold |

## Comprehensive Alert Configuration

### Account-Level Defaults
Set organization-wide alerting defaults that apply to all checks unless overridden:

```yaml
# Account-level alert settings
Escalation Strategy: Run-based (2 consecutive failures)
Reminder Schedule: 
  - First reminder: 5 minutes
  - Second reminder: 15 minutes
Alert Channels:
  - Email: team@company.com
  - Slack: #ops-alerts
Location Threshold: 50% of locations must fail
```

### Group-Level Overrides
Configure specific alerting behavior for related checks:

```yaml
# Production API Group
Escalation Strategy: Time-based (5 minutes)
Alert Channels:
  - PagerDuty: Production escalation policy
  - Slack: #production-alerts
Mute Alerts: During maintenance windows

# Development Group
Escalation Strategy: Run-based (3 consecutive failures)
Alert Channels:
  - Email: dev-team@company.com
Reminder Schedule: None
```

### Check-Level Customization
Fine-tune alerting for individual checks with specific requirements:

```yaml
# Critical payment API
Escalation Strategy: Immediate (first failure)
Alert Channels:
  - PagerDuty: Payment team escalation
  - SMS: On-call engineer
Retry Strategy: Fixed (3 retries, 10 second intervals)

# Internal monitoring dashboard
Escalation Strategy: Run-based (5 consecutive failures)
Alert Channels:
  - Email: Internal team
Mute Alerts: Enabled during development
```






## Getting Started with Alerting

<Steps>
<Step title="Configure Account Defaults">
Set up organization-wide alert settings that work for most of your checks
</Step>

<Step title="Set Up Alert Channels">
Configure email, Slack, PagerDuty, or custom webhook integrations
</Step>

<Step title="Define Retry Strategies">
Choose appropriate retry mechanisms to reduce false positives
</Step>

<Step title="Configure Group Overrides">
Set specific alerting for different teams or service categories
</Step>

<Step title="Test Your Configuration">
Use Checkly's test features to validate your alerting setup
</Step>

<Step title="Monitor and Optimize">
Review alert patterns and adjust configurations to reduce noise
</Step>
</Steps>

## Alert Management Resources

<CardGroup cols={2}>
<Card title="Alert Channels" href="/docs/communicate/alerts/channels">
Complete guide to setting up notification channels and integrations
</Card>

<Card title="Alert Configuration" href="/docs/communicate/alerts/configuration">
Detailed configuration options for escalation, retries, and thresholds
</Card>

<Card title="Webhook Integration" href="/docs/communicate/alerts/webhooks">
Advanced webhook patterns and third-party system integrations
</Card>

<Card title="Best Practices" href="/docs/communicate/alerts/best-practices">
Proven strategies for effective alerting and incident response
</Card>
</CardGroup>

<Note>
Effective alerting balances timely notification with alert fatigue prevention. Start with conservative settings and gradually tune based on your team's response patterns and service reliability requirements.
</Note>

<Tip>
Use Checkly's alert notification log to analyze your alerting patterns and identify opportunities to reduce noise while maintaining coverage of critical issues.
</Tip> 